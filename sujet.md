# Practical Session #1: Introduction

1. Find in news sources a general public article reporting the discovery of a software bug. Describe the bug. If possible, say whether the bug is local or global and describe the failure that manifested its presence. Explain the repercussions of the bug for clients/consumers and the company or entity behind the faulty program. Speculate whether, in your opinion, testing the right scenario would have helped to discover the fault.


2. Apache Commons projects are known for the quality of their code and development practices. They use dedicated issue tracking systems to discuss and follow the evolution of bugs and new features. The following link https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-794?filter=doneissues points to the issues considered as solved for the Apache Commons Collections project. Among those issues find one that corresponds to a bug that has been solved. Classify the bug as local or global. Explain the bug and the solution. Did the contributors of the project add new tests to ensure that the bug is detected if it reappears in the future?


3. Netflix is famous, among other things we love, for the popularization of *Chaos Engineering*, a fault-tolerance verification technique. The company has implemented protocols to test their entire system in production by simulating faults such as a server shutdown. During these experiments they evaluate the system's capabilities of delivering content under different conditions. The technique was described in [a paper](https://arxiv.org/ftp/arxiv/papers/1702/1702.05843.pdf) published in 2016. Read the paper and briefly explain what are the concrete experiments they perform, what are the requirements for these experiments, what are the variables they observe and what are the main results they obtained. Is Netflix the only company performing these experiments? Speculate how these experiments could be carried in other organizations in terms of the kind of experiment that could be performed and the system variables to observe during the experiments.


4. [WebAssembly](https://webassembly.org/) has become the fourth official language supported by web browsers. The language was born from a joint effort of the major players in the Web. Its creators presented their design decisions and the formal specification in [a scientific paper](https://people.mpi-sws.org/~rossberg/papers/Haas,%20Rossberg,%20Schuff,%20Titzer,%20Gohman,%20Wagner,%20Zakai,%20Bastien,%20Holman%20-%20Bringing%20the%20Web%20up%20to%20Speed%20with%20WebAssembly.pdf) published in 2018. The goal of the language is to be a low level, safe and portable compilation target for the Web and other embedding environments. The authors say that it is the first industrial strength language designed with formal semantics from the start. This evidences the feasibility of constructive approaches in this area. Read the paper and explain what are the main advantages of having a formal specification for WebAssembly. In your opinion, does this mean that WebAssembly implementations should not be tested? 


5.  Shortly after the appearance of WebAssembly another paper proposed a mechanized specification of the language using Isabelle. The paper can be consulted here: https://www.cl.cam.ac.uk/~caw77/papers/mechanising-and-verifying-the-webassembly-specification.pdf. This mechanized specification complements the first formalization attempt from the paper. According to the author of this second paper, what are the main advantages of the mechanized specification? Did it help improving the original formal specification of the language? What other artifacts were derived from this mechanized specification? How did the author verify the specification? Does this new specification removes the need for testing?

## Réponses
### Floriane HEMIDY & Thibaud MORIN

1. Cet [article](https://www.capital.fr/entreprises-marches/sncf-connect-un-nouveau-bug-fait-beaucoup-varier-le-prix-des-billets-1428963) de Capital met en avant un bug de l'application SNCF Connect, qui permet aux utilisateurs de réserver leurs billets pour tous les services SNCF. En effet, il a été découvert que l'application propose des prix différents selon la langue utilisée par l'utilisateur ou sa localisation. Peut-être qu'il s'agit d'une fonctionnalité voulue par la SNCF mais l'écart de prix est très conséquent. Pour un billet Paris-Barcelone, un Français paiera 34 euros contre 122 euros pour un belge, soit 3,5 fois plus. Il est quasiment certain qu'il s'agisse d'un bug, de configuration selon la SNCF. Pour éviter ce problème, il aurait fallu tester l'application dans différents contextes, ce qui n'a certainement pas été réalisé.


2. Dans l'[issue](https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-799?filter=doneissues) suivante, il a été détecté qu'un ```UnmodifiableNavigableSet``` est modifiable avec l'appel aux méthodes ```pollFirst()``` and ```pollLast()```. Il s'agit d'un bug local. Pour corriger ce bug, l'appel de ces méthodes provoque maintenant une exception. Les tests ont également été modifiés en conséquence. Les tests vérifient que l'appel de ces méthodes lance bien une exception comme prévu. Le test comprend aussi la vérification que le ```UnmodifiableNavigableSet``` n'est pas modifiable et que l'appel de chaque fonction de modification renvoie une exception.


3. L’expérience consiste à prendre un certain nombre d’utilisateurs et de le diviser en deux groupes. Un groupe sera le groupe témoin et l’autre le groupe expérimental. Le groupe expérimental aura une simulation d’échecs. Quant au groupe témoin, il y aura aucune modification. Le but est de vérifier, à la sortie de cette expérience, la mesure des streams par seconde pour quantifier l’impact des échecs. Pour l’expérience, il faut un panel d’utilisateurs. Pour simuler les échecs, l’idée est de créer des événements similaires à l’activité réelle de Netflix. Par exemple, ils peuvent simuler des défauts sur des serveurs ou des disques durs ou encore une connexion réseau coupée. Le résultat va nous permettre de signaler deux possibilités. Soit il y a un dysfonctionnement au niveau du système ce qui signale une mauvaise gestion des événements simulés, soit une faiblesse a été découverte ce qui fait qu’il faudra la faire progresser. Plusieurs organisations utilisent cette méthode d’expérience. Pour que ces expériences soient adaptées à d’autres sociétés, il faudra mettre en place des outils pour mesurer le résultat des expériences et il faudra rechercher un catalogue d’événements à réaliser pour déclencher les échecs.


4. WebAssembly possède une spécification formelle. Cela permet d'avoir une spécification claire mathématiquement. Parfois, avec une spécification informelle, elle peut être incomplète mais on ne s'en rend pas compte. La spécification formelle permet de vérifier qu'elle est bien complète. Cependant, cela n'exclut pas la nécessité de la tester. C'est d'ailleurs la partie la plus importante, car la spécification formelle permet de réaliser des tests rigoureux. Il peut être pertinent de vérifier formellement cette spécification, par exemple en utilisant Isabelle HOL.


5. La spécification mécanisée permet de trouver des erreurs éventuelles dans la spécification formelle du langage WebAssembly. Celle-ci a permis de soulever certaines erreurs, qui ont pu être signalées et corrigées. Nous pouvons donc dire que la spécification formelle originale a pu être améliorée. L'artefact JSCert a également pu être vérifié, ce qui a permis de montrer qu'il avait une grosse dette technique. Ceci ne supprime en rien la nécessité de tester la spécification.